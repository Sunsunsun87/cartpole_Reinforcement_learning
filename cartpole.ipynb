{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from random import sample, random\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "# import torchsnooper\n",
    "\n",
    "@dataclass\n",
    "class Sarsd:\n",
    "    state: Any \n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: Any\n",
    "    done: bool  # \n",
    "        \n",
    "class DQN_Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def get_actions(self, observations):\n",
    "        # observations shape is (N, 4) (x, x', the, omega)\n",
    "        q_values = self.model(observations)\n",
    "        \n",
    "        # q_values shape(N, 2) (left, right)?\n",
    "         \n",
    "        return q_values.max(-1)[1]  # \n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, obs_shape, num_actions):\n",
    "        super(Model, self).__init__()\n",
    "        assert len(obs_shape) == 1, \"This network only works for flat observations\"\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.net = torch.nn.Sequential(   #\n",
    "            torch.nn.Linear(obs_shape[0], 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, num_actions),\n",
    "        )\n",
    "        self.opt = optim.Adam(self.net.parameters(), lr = 0.0001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)        #在train_step里是正经的forward pass--系统了解forward pass!\n",
    "\n",
    "    \n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size = 100000):  \n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen = buffer_size)\n",
    "        \n",
    "    def insert(self, sarsd):\n",
    "        self.buffer.append(sarsd)\n",
    "#         self.buffer = self.buffer[-self.buffer_size:] #   \n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        assert num_samples <= len(self.buffer)\n",
    "        return sample(self.buffer, num_samples)\n",
    "    \n",
    "    \n",
    "def update_tgt_model(m, tgt):\n",
    "    tgt.load_state_dict(m.state_dict())\n",
    "    \n",
    "    \n",
    "#@torchsnooper.snoop()\n",
    "def train_step(model, state_transitions, tgt, num_actions):        # get the state vector here ********\n",
    "    cur_states = torch.stack([torch.Tensor(s.state) for s in state_transitions])\n",
    "    rewards = torch.stack([torch.Tensor([s.reward]) for s in state_transitions])\n",
    "    mask = torch.stack(([torch.tensor([0], dtype = torch.float32) if s.done else torch.Tensor([1] ) for s in state_transitions]))\n",
    "    next_states = torch.stack([torch.Tensor(s.next_state) for s in state_transitions])\n",
    "    actions = [s.action for s in state_transitions]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvals_next = tgt(next_states).max(-1)[0]\n",
    "        #print(qvals_next.shape)\n",
    "    model.opt.zero_grad()\n",
    "    qvals = model(cur_states) # (N, num_actions)\n",
    "#     print(qvals.shape()) 2500\n",
    "#     print(actions)\n",
    "    one_hot_actions = F.one_hot(torch.LongTensor(actions), num_actions)\n",
    "    print(one_hot_actions.size)\n",
    "#     print(one_hot_actions)\n",
    "#     import ipdb; ipdb.set_trace() \n",
    "#     print(type(rewards), type(mask), type(qvals_next), type(qvals), type(one_hot_actions))\n",
    "    x = qvals * one_hot_actions.to(torch.float32)\n",
    "    print(x)\n",
    "    loss = ((rewards + mask[:, 0] * qvals_next - torch.sum(x, -1))**2).mean()\n",
    "    loss.backward()\n",
    "    model.opt.step()\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def main(test = False, chkpt = None):\n",
    "    if not test:\n",
    "        wandb.init(project = \"dqn-tutorial\", name = \"dqn-cartpole\")\n",
    "    min_rb_size = 10000\n",
    "    sample_size = 2500\n",
    "    \n",
    "#     eps_max = 1.0\n",
    "    exp_min = 0.01\n",
    "    \n",
    "    eps_decay = 0.99998\n",
    "    \n",
    "    env_steps_before_train = 100  # every 100 steps, train 2500 samples;\n",
    "    tgt_model_update = 150 # epochs\n",
    "    \n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    last_observation = env.reset()\n",
    "    \n",
    "    m = Model(env.observation_space.shape, env.action_space.n)\n",
    "    if chkpt is not None:\n",
    "        m.load_state_dict(torch.load(chkpt))\n",
    "    tgt = Model(env.observation_space.shape, env.action_space.n)\n",
    "    update_tgt_model(m, tgt)\n",
    "    \n",
    "    rb = ReplayBuffer()\n",
    "    steps_since_former_train = 0\n",
    "    epochs_since_tgt = 0\n",
    "    \n",
    "    step_num = -1 * min_rb_size\n",
    "    #qvals = m(torch.Tensor(observation))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    rolling_reward = 0\n",
    "\n",
    "    tq = tqdm()\n",
    "    try:\n",
    "        while True:\n",
    "            if test:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            tq.update(1)\n",
    "            \n",
    "            eps = eps_decay**(step_num)\n",
    "            if test:\n",
    "                eps = 0\n",
    "                \n",
    "            if random() < eps:\n",
    "                \n",
    "                action = env.action_space.sample() # Your agent here takes random actions\n",
    "#                 print(action)\n",
    "            else: \n",
    "                action = m(torch.Tensor(last_observation)).max(-1)[1].item()\n",
    "                \n",
    "            observation, reward, done, info = env.step(action)\n",
    "            rolling_reward += reward\n",
    "    #    env.render()\n",
    "    #    time.sleep(0.1)  \n",
    "    #    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    #    observation, reward, done, info = env.step(action)\n",
    "    #\n",
    "    \n",
    "            reward = reward / 100.0 # Normalization\n",
    "        \n",
    "            rb.insert(Sarsd(last_observation, action, reward, observation, done))\n",
    "#             print(rb.buffer[10].done)\n",
    "#             import ipdb; ipdb_set_trace()\n",
    "            last_observation = observation\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(rolling_reward)\n",
    "                if test:\n",
    "                    print(rolling_reward)\n",
    "                rolling_reward = 0\n",
    "                observation = env.reset()\n",
    "                \n",
    "            steps_since_former_train += 1\n",
    "            step_num += 1\n",
    "            \n",
    "            if not test and len(rb.buffer) > min_rb_size and steps_since_former_train > env_steps_before_train:         #不立马update, 等一会才\n",
    "                loss = train_step(m, rb.sample(sample_size), tgt, env.action_space.n)\n",
    "                wandb.log({'loss': loss.detach().item(), 'eps': eps, 'avg_reward': np.mean(episode_rewards), \\\n",
    "                          }, step = step_num)\n",
    "#                 print(step_num, loss.detach().item())\n",
    "                episode_rewards = []\n",
    "                epochs_since_tgt += 1\n",
    "                if epochs_since_tgt > tgt_model_update:\n",
    "                    print(\"updating target model\")\n",
    "                    update_tgt_model(m, tgt)\n",
    "                    epochs_since_tgt = 0\n",
    "                    torch.save(tgt.state_dict(), f\"D:/college/machine_learning/Jack of Some's cartpole tut/models_cartpole/ \\\n",
    "                    {step_num}.pth\")\n",
    "                    \n",
    "                steps_since_former_train = 0\n",
    "                \n",
    "#                 import ipdb; ipdb.set_trace()\n",
    "#                 print(loss)\n",
    "#                 raise Exception()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(True, )#\"D:/college/machine_learning/Jack of Some's cartpole tut/models_cartpole/                     228665.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
